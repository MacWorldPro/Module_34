{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNalEvA2hcVmwcGy/Elho2Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MacWorldPro/Module_34/blob/main/Module51.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sure, let's address each question:\n",
        "\n",
        "Q1) **Overfitting:** Overfitting occurs when a model learns the training data too well, capturing noise or random fluctuations rather than the underlying pattern. Consequences include poor generalization to unseen data, high variance, and reduced model interpretability. To mitigate overfitting, techniques such as cross-validation, regularization, and using more data can be employed.\n",
        "\n",
        "   **Underfitting:** Underfitting occurs when a model is too simple to capture the underlying structure of the data. Consequences include poor performance on both training and test data and high bias. To mitigate underfitting, one can use more complex models, feature engineering, or increasing model capacity.\n",
        "\n",
        "Q2) To reduce overfitting, one can:\n",
        "   - Use regularization techniques like L1 or L2 regularization.\n",
        "   - Employ dropout, a technique where random neurons are temporarily removed during training.\n",
        "   - Collect more data to make the model generalize better.\n",
        "   - Simplify the model architecture.\n",
        "   - Use cross-validation to tune hyperparameters effectively.\n",
        "\n",
        "Q3) Underfitting occurs when a model is too simple to capture the underlying patterns in the data. Scenarios where underfitting can occur include:\n",
        "   - Using a linear model for a non-linear relationship.\n",
        "   - Insufficient training data.\n",
        "   - High levels of noise in the data.\n",
        "\n",
        "Q4) **Bias-Variance Tradeoff:** Bias refers to the error introduced by approximating a real-world problem with a simplified model. Variance refers to the model's sensitivity to fluctuations in the training data. The bias-variance tradeoff states that as you decrease bias (by increasing model complexity), you often increase variance, and vice versa. Both bias and variance affect model performance: high bias leads to underfitting, while high variance leads to overfitting.\n",
        "\n",
        "Q5) Common methods for detecting overfitting and underfitting include:\n",
        "   - Cross-validation: Assessing model performance on multiple subsets of the data.\n",
        "   - Learning curves: Plotting training and validation error against training set size.\n",
        "   - Validation curves: Plotting model performance against hyperparameter values.\n",
        "   - Analyzing residuals: Checking for patterns or systematic errors in model predictions.\n",
        "\n",
        "Q6) **Bias vs. Variance:**\n",
        "   - High bias models are too simple and fail to capture the underlying patterns in the data, leading to underfitting. Examples include linear regression on non-linear data.\n",
        "   - High variance models are too complex and capture noise or random fluctuations in the training data, leading to overfitting. Examples include decision trees with no depth constraint.\n",
        "\n",
        "Q7) **Regularization:** Regularization is a technique used to prevent overfitting by adding a penalty term to the loss function, discouraging overly complex models. Common regularization techniques include:\n",
        "   - L1 regularization (Lasso): Adds the absolute value of the coefficients as a penalty term.\n",
        "   - L2 regularization (Ridge): Adds the square of the coefficients as a penalty term.\n",
        "   - Dropout: Randomly deactivating neurons during training to prevent co-adaptation.\n",
        "   - Early stopping: Stopping the training process when the performance on a validation set starts to degrade."
      ],
      "metadata": {
        "id": "VXzjXbv0TJVe"
      }
    }
  ]
}